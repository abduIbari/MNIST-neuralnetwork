# MNIST-neuralnetwork

This project implements a simple fully connected neural network from scratch using NumPy to classify digits from the MNIST dataset. The neural network uses ReLU activation for the hidden layers and softmax for the output layer.

### Overview
The neural network is built with two hidden layers using **ReLU** as well as **Sigmoid** activation and an output layer with softmax. It trains via backpropagation and gradient descent with a configurable learning rate and number of iterations.

- Input Layer: 784 neurons (28x28 pixels)
- Hidden Layer 1: 128 neurons (ReLU)
- Hidden Layer 2: 256 neurons (ReLU)
- Output Layer: 10 neurons (softmax)

Link to download the dataset: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv

